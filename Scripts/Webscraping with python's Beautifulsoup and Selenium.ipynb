{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ecae73",
   "metadata": {},
   "source": [
    "# Webscrapping News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8cdf7d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Working with Post-Courier Page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948a00a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Scraping each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee96f5e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "# opening a browser window\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# --- 1. User Settings (FILL THESE IN) ---\n",
    "LOGIN_URL = \"https://www.postcourier.com.pg/my-account/\"\n",
    "YOUR_USERNAME = \"username\"\n",
    "YOUR_PASSWORD = \"password\"\n",
    "\n",
    "STARTING_STORIES_URL = \"https://www.postcourier.com.pg/top-stories/\"\n",
    "MAX_PAGES_TO_SCRAPE = 3  # Set to 1 to only scrape the first page\n",
    "\n",
    "# --- 2. Set up Selenium Driver ---\n",
    "print(\"Connecting to browser...\")\n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver, 10) # 10-second wait time\n",
    "\n",
    "# --- 3. Perform Login ---\n",
    "try:\n",
    "    print(f\"Opening login page: {LOGIN_URL}\")\n",
    "    driver.get(LOGIN_URL)\n",
    "    \n",
    "    username_field = wait.until(EC.visibility_of_element_located((By.ID, \"username\")))\n",
    "    password_field = driver.find_element(By.ID, \"password\")\n",
    "    login_button = driver.find_element(By.NAME, \"login\")\n",
    "\n",
    "    print(\"Logging in...\")\n",
    "    username_field.send_keys(YOUR_USERNAME)\n",
    "    password_field.send_keys(YOUR_PASSWORD)\n",
    "    login_button.click()\n",
    "\n",
    "    wait.until(EC.visibility_of_element_located((By.CLASS_NAME, \"woocommerce-MyAccount-content\")))\n",
    "    print(\"Login Successful!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Login failed: {e}\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# --- 4. NEW: PAGINATION LOOP (Phase 1) ---\n",
    "print(\"\\n--- Phase 1: Collecting all article links ---\")\n",
    "driver.get(STARTING_STORIES_URL)\n",
    "\n",
    "# This is our master list that will hold all articles from all pages\n",
    "all_article_data = []\n",
    "pages_scraped = 0\n",
    "\n",
    "while pages_scraped < MAX_PAGES_TO_SCRAPE:\n",
    "    pages_scraped += 1\n",
    "    print(f\"\\nScraping page {pages_scraped}...\")\n",
    "    \n",
    "    # Wait for the main content to load\n",
    "    try:\n",
    "        wait.until(EC.visibility_of_element_located((By.ID, \"main\")))\n",
    "    except TimeoutException:\n",
    "        print(\"  Error: Page timed out. Stopping.\")\n",
    "        break\n",
    "\n",
    "    # Pass the current page source to Beautiful Soup\n",
    "    page_html = driver.page_source\n",
    "    soup = BeautifulSoup(page_html, \"html.parser\")\n",
    "\n",
    "    main_content = soup.find(\"main\", id=\"main\")\n",
    "    articles = main_content.find_all(\"article\") if main_content else []\n",
    "    \n",
    "    articles_found_on_page = 0\n",
    "    for article in articles:\n",
    "        title_tag = article.find(\"h2\", class_=\"entry-title\")\n",
    "        date_tag = article.find(\"time\", class_=\"entry-date published\")\n",
    "        \n",
    "        if title_tag and title_tag.a and date_tag:\n",
    "            all_article_data.append({\n",
    "                \"title\": title_tag.a.text,\n",
    "                \"url\": title_tag.a['href'],\n",
    "                \"date\": date_tag.text,\n",
    "                \"full_text\": \"\" # Placeholder\n",
    "            })\n",
    "            articles_found_on_page += 1\n",
    "\n",
    "    print(f\"  Found {articles_found_on_page} articles on this page.\")\n",
    "\n",
    "    # --- Find and click the 'Next' button ---\n",
    "    try:\n",
    "        # Find the 'Older posts' link\n",
    "        next_button = driver.find_element(By.CSS_SELECTOR, \"a.next.page-numbers\")\n",
    "        \n",
    "        print(\"  Clicking 'Older posts' button...\")\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button) # Scroll to button\n",
    "        time.sleep(0.5)\n",
    "        next_button.click()\n",
    "        \n",
    "        # Wait for the new page to load (e.g., wait for the main element again)\n",
    "        time.sleep(2) # Give it a moment to navigate\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        print(\"  No 'Older posts' button found. This is the last page.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"  Error clicking next button: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n--- Phase 1 Complete: Collected {len(all_article_data)} total articles ---\")\n",
    "\n",
    "\n",
    "# --- 5. SCRAPING LOOP (Phase 2) ---\n",
    "print(\"\\n--- Phase 2: Scraping full text for each article ---\")\n",
    "\n",
    "for article in all_article_data: \n",
    "    try:\n",
    "        print(f\"Fetching: {article['title']}\")\n",
    "        \n",
    "        driver.get(article['url'])\n",
    "        wait.until(EC.visibility_of_element_located((By.CLASS_NAME, \"entry-content\")))\n",
    "        \n",
    "        article_html = driver.page_source\n",
    "        article_soup = BeautifulSoup(article_html, \"html.parser\")\n",
    "        content_div = article_soup.find(\"div\", class_=\"entry-content\")\n",
    "        \n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all(\"p\")\n",
    "            full_text = \"\\n\".join([p.text for p in paragraphs])\n",
    "            article['full_text'] = full_text\n",
    "            print(\"  ...Success! Scraped p-tag content.\")\n",
    "        else:\n",
    "            print(\"  ...Could not find 'entry-content' div.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error fetching {article['url']}: {e}\")\n",
    "\n",
    "# --- 6. Clean up Selenium ---\n",
    "driver.quit()\n",
    "print(\"\\nSelenium driver closed.\")\n",
    "\n",
    "# --- 7. Create and Display the Table ---\n",
    "print(\"\\n--- Scraping Complete. Creating Table ---\")\n",
    "\n",
    "df = pd.DataFrame(all_article_data)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# --- 8. Save the table to a file ---\n",
    "try:\n",
    "    output_filename = \"scraped_articles_all_pages.csv\"\n",
    "    df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\nSuccessfully saved data to {output_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving to CSV: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9843950a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Scraping with CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d75bc3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- 1. User Settings (FILL THESE IN) ---\n",
    "LOGIN_URL = \"https://www.postcourier.com.pg/my-account/\"\n",
    "YOUR_USERNAME = \"charlieikosi@gmail.com\"\n",
    "YOUR_PASSWORD = \"cikosi26490!\"\n",
    "\n",
    "# *** SET YOUR CSV FILENAME HERE ***\n",
    "INPUT_CSV_FILE = \"national_articles_pc.csv\"  # e.g., \"articles_to_scrape.csv\"\n",
    "OUTPUT_CSV_FILE = \"articles_with_full_text.csv\"\n",
    "\n",
    "# --- 2. Load the CSV File ---\n",
    "print(f\"Loading data from {INPUT_CSV_FILE}...\")\n",
    "try:\n",
    "    # Read the CSV into a Pandas DataFrame\n",
    "    df = pd.read_csv(INPUT_CSV_FILE)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found.\")\n",
    "    print(f\"Please make sure '{INPUT_CSV_FILE}' is in the same folder as this script.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Check for the required 'URL' column\n",
    "if 'URL' not in df.columns:\n",
    "    print(f\"Error: Your CSV must have a column named 'URL'.\")\n",
    "    print(f\"Found columns: {df.columns.to_list()}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found {len(df)} URLs to scrape.\")\n",
    "\n",
    "# --- 3. Set up Selenium Driver ---\n",
    "print(\"Connecting to browser...\")\n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver, 10) # 10-second wait time\n",
    "\n",
    "# --- 4. Perform Login ---\n",
    "try:\n",
    "    print(f\"Opening login page: {LOGIN_URL}\")\n",
    "    driver.get(LOGIN_URL)\n",
    "    \n",
    "    username_field = wait.until(EC.visibility_of_element_located((By.ID, \"username\")))\n",
    "    password_field = driver.find_element(By.ID, \"password\")\n",
    "    login_button = driver.find_element(By.NAME, \"login\")\n",
    "\n",
    "    print(\"Logging in...\")\n",
    "    username_field.send_keys(YOUR_USERNAME)\n",
    "    password_field.send_keys(YOUR_PASSWORD)\n",
    "    login_button.click()\n",
    "\n",
    "    wait.until(EC.visibility_of_element_located((By.CLASS_NAME, \"woocommerce-MyAccount-content\")))\n",
    "    print(\"Login Successful!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Login failed: {e}\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# --- 5. SCRAPING LOOP (Using the CSV) ---\n",
    "print(\"\\n--- Starting to Scrape Full Articles ---\")\n",
    "\n",
    "# This list will store the scraped text for each row\n",
    "scraped_texts = [] \n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    \n",
    "    # Use 'Top.Stories' for the print message, or the URL if it's not there\n",
    "    title = row.get('Top.Stories', url) \n",
    "    \n",
    "    try:\n",
    "        print(f\"Fetching ({index + 1}/{len(df)}): {title}\")\n",
    "        \n",
    "        driver.get(url)\n",
    "        # Wait for the main article content to load\n",
    "        wait.until(EC.visibility_of_element_located((By.CLASS_NAME, \"entry-content\")))\n",
    "        \n",
    "        article_html = driver.page_source\n",
    "        article_soup = BeautifulSoup(article_html, \"html.parser\")\n",
    "        \n",
    "        # Find the content div\n",
    "        content_div = article_soup.find(\"div\", class_=\"entry-content\")\n",
    "        \n",
    "        if content_div:\n",
    "            # Get text from <p> tags\n",
    "            paragraphs = content_div.find_all(\"p\")\n",
    "            full_text = \"\\n\".join([p.text for p in paragraphs])\n",
    "            scraped_texts.append(full_text)\n",
    "            print(\"  ...Success! Scraped p-tag content.\")\n",
    "        else:\n",
    "            print(\"  ...Could not find 'entry-content' div.\")\n",
    "            scraped_texts.append(\"[Scrape Error: No content div found]\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error fetching {url}: {e}\")\n",
    "        scraped_texts.append(f\"[Scrape Error: {e}]\")\n",
    "\n",
    "# --- 6. Clean up Selenium ---\n",
    "driver.quit()\n",
    "print(\"\\nSelenium driver closed.\")\n",
    "\n",
    "# --- 7. Add Scraped Data to DataFrame ---\n",
    "print(\"\\n--- Scraping Complete. Adding new column to table ---\")\n",
    "\n",
    "# Add the list of texts as a new column in our original DataFrame\n",
    "df['full_text'] = scraped_texts\n",
    "\n",
    "# Set display options to show the table in the console\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# --- 8. Save the new table to a file ---\n",
    "try:\n",
    "    df.to_csv(OUTPUT_CSV_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"\\nSuccessfully saved new data to {OUTPUT_CSV_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving to CSV: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119e44d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Working with TVWAN Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ec24b8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "wait = WebDriverWait(driver, 10) \n",
    "\n",
    "# Lists to store our data\n",
    "titles = []\n",
    "urls = []\n",
    "article_dates = []\n",
    "total_page = len(links)\n",
    "page_count = 0\n",
    "\n",
    "try:\n",
    "    #driver.get(\"https://tvwan.com.pg/\")\n",
    "    #driver.maximize_window()\n",
    "    \n",
    "    # Wait for the \"News\" link and click it\n",
    "    #wait.until(EC.element_to_be_clickable((By.CLASS_NAME, \"menu-item-link\"))).click()\n",
    "    \n",
    "    # Wait for the news articles to be present\n",
    "    #wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"feature-news-content\")))\n",
    "    links = driver.find_elements(By.CLASS_NAME, \"feature-news-content\")\n",
    "    \n",
    "    print(f\"Found {len(links)} links. Gathering info...\")\n",
    "\n",
    "    # --- LOOP 1: GATHER ALL TITLES AND URLS ---\n",
    "    for link in links:\n",
    "        # Get Article Title\n",
    "        titles.append(link.text)\n",
    "        \n",
    "        # Get URL\n",
    "        link_tag = link.find_element(By.TAG_NAME, 'a')\n",
    "        url_link = link_tag.get_attribute('href')\n",
    "        urls.append(url_link)\n",
    "\n",
    "    print(\"Finished gathering links. Now visiting each page...\")\n",
    "\n",
    "    # --- LOOP 2: VISIT EACH URL TO SCRAPE THE DATE ---\n",
    "    for url in urls:\n",
    "        page_count +=1\n",
    "        print(f\"Visiting: {url} Page: {page_count} of {total_page}.\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        try:\n",
    "            # Wait for the date element to load on the article page\n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".author-info li\")))\n",
    "            \n",
    "            # Get page source *after* waiting\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Select the date element\n",
    "            article_date_element = soup.select_one(\".author-info li\")\n",
    "            \n",
    "            if article_date_element:\n",
    "                clean_date = article_date_element.text.strip()\n",
    "                article_dates.append(clean_date)\n",
    "            else:\n",
    "                article_dates.append(\"Date not found\") # Append a placeholder\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Could not get date from {url}. Error: {e}\")\n",
    "            article_dates.append(\"Error\") # Append error placeholder\n",
    "    \n",
    "    print(\"--- Scraping Complete ---\")\n",
    "    print(\"Titles:\", titles)\n",
    "    print(\"URLs:\", urls)\n",
    "    print(\"Dates:\", article_dates)\n",
    "\n",
    "finally:\n",
    "    # --- QUIT ONCE AT THE VERY END ---\n",
    "    print(\"All done. Closing browser.\")\n",
    "    driver.quit()\n",
    "\n",
    "scraped_articles_df = pd.DataFrame({\n",
    "    \"Title\": titles,\n",
    "    \"Date\": article_dates,\n",
    "    \"URL\": urls\n",
    "})\n",
    "\n",
    "# See datatable\n",
    "scraped_articles_df\n",
    "\n",
    "# Export DataFrame to CSV\n",
    "scraped_articles_df.to_csv('tvwan_local.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c50177c",
   "metadata": {},
   "source": [
    "## Analysis with GEMINI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e47367e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import pandas as pd\n",
    "# Option 1: Set as an environment variable (Recommended)\n",
    "# In your terminal: export GEMINI_API_KEY=\"YOUR_API_KEY\"\n",
    "# In your code:\n",
    "# genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Option 2: Hardcode it (For quick testing only)\n",
    "genai.configure(api_key=\"apikey\")\n",
    "\n",
    "# Select the model you want to use\n",
    "# 'gemini-2.5-flash' is fast and cost-effective\n",
    "model = genai.GenerativeModel('gemini-2.5-pro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d67dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "INPUT_CSV_FILE = \"articles_with_full_text.csv\"\n",
    "\n",
    "# Read the CSV into a Pandas DataFrame\n",
    "df = pd.read_csv(INPUT_CSV_FILE)\n",
    "df = df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8b963c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 1 appended\n",
      "Sentiment 2 appended\n",
      "Sentiment 3 appended\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'done' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m     sentiment_list.append(response)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSentiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m appended\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdone\u001b[49m)   \n",
      "\u001b[31mNameError\u001b[39m: name 'done' is not defined"
     ]
    }
   ],
   "source": [
    "#model = genai.GenerativeModel('gemini-2.5-pro')\n",
    "sentiment_list = []\n",
    "count = 0\n",
    "base_prompt = \"You are going to tag the article with the relevant category that it talks about. You response must not explain or give details. You must just return the word for the tag , followed it's sentiment. Therefore i only expect one word for the tag and one for the sentiment\"\n",
    "for article in df['full_text']:\n",
    "    count += 1\n",
    "    # Combine your instructions with the article\n",
    "    full_prompt = [base_prompt,\"Here is the article:\",article]\n",
    "    response = model.generate_content(full_prompt)\n",
    "    sentiment_list.append(response)\n",
    "    print(f\"Sentiment {count} appended\")\n",
    "print('done')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a562d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education Positive\n",
      "Government Positive\n",
      "Politics Neutral\n"
     ]
    }
   ],
   "source": [
    "for i in sentiment_list:\n",
    "    print(i.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrapping",
   "language": "python",
   "name": "webscrapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
